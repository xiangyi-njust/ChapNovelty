{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优化章节名和参考文献名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 章节名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55580\n",
      "55580\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../Data/plos_section.csv\")\n",
    "sections = df['section'].tolist()\n",
    "\n",
    "print(len(sections))\n",
    "sections = list(set(sections))\n",
    "print(len(sections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " 'Methodological Considerations and Study Limitations',\n",
       " '2. Flow field model of micro groove seal',\n",
       " '9 Supplementary materials',\n",
       " 'Ethical approval and informed consent']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    1. 对于不规范但确实属于章节名的数据\n",
    "    2. 对于不规范且不属于章节名的数据  （可能是爬取过程中存在的问题，这部分数据如何处理，去除吗？）\n",
    "\n",
    "    但重点是如何利用章节名信息🤔， 比如说我认为在method部分共现的更加重要，这依据何在？\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 参考文献名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    1. 通过建立字典，去优化期刊名\n",
    "    2. 通过从WOS Science进行检索获取参考文献\n",
    "\n",
    "    这一步是肯定要做的,且这一步骤是需要针对所有论文进行处理的\n",
    "\"\"\"\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13024187, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/refer_source.csv')\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = df['Refer_Source'].tolist()\n",
    "\n",
    "title_to_nums = {}\n",
    "for title in sources:\n",
    "    if type(title) != str:\n",
    "        continue\n",
    "    if title not in title_to_nums:\n",
    "        title_to_nums[title] = 0\n",
    "    else:\n",
    "        title_to_nums[title] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887593\n"
     ]
    }
   ],
   "source": [
    "unique_sources = title_to_nums.keys()\n",
    "print(len(unique_sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match = pd.read_csv(\"data/match_journal.csv\")\n",
    "match_journals = df_match['MATCH Source'].tolist()\n",
    "\n",
    "cnt = 0\n",
    "error_cnt = 0\n",
    "error_journals = []\n",
    "for journal in set(match_journals):\n",
    "    try:\n",
    "        cnt += title_to_nums[journal]\n",
    "    except:\n",
    "        error_journals.append(journal)\n",
    "        error_cnt += 1\n",
    "\n",
    "print(cnt)\n",
    "print(cnt/len(sources))\n",
    "print(error_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir(\"data/standard journal list\")\n",
    "\n",
    "journals = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(\"data/standard journal list\", file))\n",
    "    try:\n",
    "        journals.extend(df['Journal title'].tolist())\n",
    "    except:\n",
    "        journals.extend(df['Title'].tolist())\n",
    "        continue\n",
    "\n",
    "scopus_df = pd.read_excel(\"data/standard journal(scopus)/ext_list_August_2024.xlsx\")\n",
    "scopus_df = scopus_df[scopus_df['Source Type'] == 'Journal']\n",
    "\n",
    "journals.extend(scopus_df['Source Title'].tolist())\n",
    "len(set(journals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68121"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_standard = pd.DataFrame()\n",
    "# df_standard['Source'] = list(set(journals))\n",
    "# df_standard.to_csv(\"data/standard_journals.csv\", index=False)\n",
    "\n",
    "df_standard = pd.read_csv(\"data/standard_journals.csv\")\n",
    "stan_journals = df_standard['Source'].tolist()\n",
    "len(set(stan_journals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99302"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 来自SciSciNet中的期刊名\n",
    "\n",
    "with open(\"data/sciscinet/SciSciNet_Journals.tsv\", 'r', encoding='utf-8') as f:\n",
    "    datas = f.readlines()\n",
    "\n",
    "sci_journals = []\n",
    "for data in datas:\n",
    "    elems = data.split('\\t')\n",
    "    sci_journals.append(elems[1])\n",
    "\n",
    "stan_journals = set(stan_journals + sci_journals)\n",
    "print(len(stan_journals))\n",
    "\n",
    "df_standard = pd.DataFrame()\n",
    "df_standard['Source'] = list(set(stan_journals))\n",
    "df_standard.to_csv(\"data/standard_journals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245324\n"
     ]
    }
   ],
   "source": [
    "# 来自openalex的期刊名\n",
    "import json\n",
    "\n",
    "openalex_journal = []\n",
    "with open(\"data/openalex/journal\", 'r') as f:\n",
    "    datas = json.load(f)\n",
    "\n",
    "openalex_journal = [data['display_name'] for data in datas]\n",
    "print(len(set(openalex_journal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/standard_journals.csv\")\n",
    "stan_journals = df['Source'].tolist()\n",
    "\n",
    "set(stan_journals).issubset(set(openalex_journal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302923\n"
     ]
    }
   ],
   "source": [
    "stan_journals = set(stan_journals+openalex_journal)\n",
    "print(len(stan_journals))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Source'] = list(stan_journals)\n",
    "df.to_csv(\"data/standard_journals_plus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Journal of Psycholinguistic Research\" in stan_journals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 标准化参考文献名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 自己构建缩写名称"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种期刊名缩写格式: \n",
    "\n",
    "https://www.sohu.com/a/673180484_121252874\n",
    "\n",
    "https://www.lcgdbzz.org/news/bkgf/58444b6f-f790-4e2d-99d2-38b932ef9ddd.htm\n",
    "\n",
    "    原： JOURNAL OF CONTROLLED RELEASE\n",
    "\n",
    "    缩写（1） J CONTROL RELEASE\n",
    "    缩写（2） J.Control. Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "func_words = ['The', 'of', 'the']\n",
    "\n",
    "word_to_abbr = {}\n",
    "\n",
    "def generate_abbreviation(title):\n",
    "    # 首字母缩写： British Medical Journal   BMJ\n",
    "    words = title.strip().split(\" \")\n",
    "    if len(words) == 1:\n",
    "        return None\n",
    "    \n",
    "    word_list = [word[0] for word in words]\n",
    "    abbr_1 = \"\".join(word_list)\n",
    "\n",
    "    # 去掉虚词后的首字母缩写\n",
    "    new_word_list = []\n",
    "    for word in words:\n",
    "        if word in func_words:\n",
    "            continue\n",
    "        else:\n",
    "            new_word_list.append(word[0])\n",
    "    abbr_2 = \"\".join(new_word_list)\n",
    "    \n",
    "    # 去掉虚词并将词语部分缩写\n",
    "    new_word_list = []\n",
    "    for word in words:\n",
    "        if word in func_words or len(word)<=5:\n",
    "            new_word_list.append(word)\n",
    "            continue\n",
    "        if word in word_to_abbr:\n",
    "            new_word_list.append(word_to_abbr[word])\n",
    "        elif word[-3:] == 'ogy' or word[-3:] == 'ics':\n",
    "            new_word_list.append(word[:-3])\n",
    "            word_to_abbr[word] = word[:-3]\n",
    "        elif word[-3:] == 'try':\n",
    "            new_word_list.append(word[:-5])\n",
    "            word_to_abbr[word] = word[:-5]\n",
    "        else:\n",
    "            min_idx = 100\n",
    "            for vowel in vowels:\n",
    "                idx = word.find(vowel)\n",
    "                if idx != -1 and idx != 0 and idx < min_idx:\n",
    "                    min_idx = idx\n",
    "            new_word_list.append(word[:min_idx])\n",
    "            word_to_abbr[word] = word[:min_idx]\n",
    "\n",
    "    abbr_3 = \" \".join(new_word_list)\n",
    "\n",
    "    return abbr_1, abbr_2, abbr_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 基于编辑距离算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可用第三方库： python-Levenshtein, editdistance, textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将结果转换为字典，便于后续代码中检索\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"data/close_form_journal.csv\")\n",
    "frame_titles = [\"Raw J\", \"New D\", \"New J\", \"New D Second\", \"New J Second\"]\n",
    "\n",
    "word_to_close_form = {}\n",
    "for row in df.itertuples():\n",
    "    \"\"\"\n",
    "        编辑距离相差太远的并不能归为同一类\n",
    "    \"\"\"\n",
    "    try:\n",
    "        word_to_close_form[row[2]] = []\n",
    "        if row[3] < 10:\n",
    "            word_to_close_form[row[2]].append(row[4])\n",
    "        if row[5] < 10:\n",
    "            word_to_close_form[row[2]].append(row[6])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "json_datas = json.dumps(word_to_close_form, indent=4)\n",
    "with open(\"data/close_form_journal.json\", \"w\") as f:\n",
    "    f.write(json_datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 从NLM官网爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/nlm/part_3_test_results.json\", 'r') as f:\n",
    "    datas = json.load(f)\n",
    "\n",
    "type(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99242\n",
      "99302\n"
     ]
    }
   ],
   "source": [
    "# 首先根据字典的键检查有哪些url是没有成功爬取下来的\n",
    "import pandas as pd\n",
    "\n",
    "crawl_urls = list(datas.keys())\n",
    "\n",
    "df = pd.read_csv(\"data/nlm/standard_journals_nlm_url.csv\")\n",
    "all_urls = df['URL'].tolist()\n",
    "\n",
    "print(len(set(all_urls)-set(crawl_urls)))\n",
    "print(len(set(all_urls)))\n",
    "\n",
    "# miss_df = pd.DataFrame()\n",
    "# miss_df['URL'] = list(set(all_urls)-set(crawl_urls))\n",
    "# miss_df.to_csv(\"data/nlm/miss_urls\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# 然后根据字典的值构建标准期刊名与其缩写之间的关系\n",
    "from tqdm import tqdm\n",
    "\n",
    "abbr_dict = {}\n",
    "\n",
    "for items in tqdm(datas.values()):\n",
    "    titles = items['title']\n",
    "    try:\n",
    "        attrs = items['attri']\n",
    "    except:\n",
    "        attrs = items['attr']\n",
    "\n",
    "    if type(titles) == str:\n",
    "        abbr_dict[titles] = attrs\n",
    "    else:\n",
    "        simple_attrs = []\n",
    "        for attr in attrs:\n",
    "            if \"NLM Title Abbreviation:\" in attr:\n",
    "                attr = attr.replace(\"NLM Title Abbreviation:\",\"\").strip()\n",
    "                simple_attrs.append(attr)\n",
    "        try:\n",
    "            assert len(simple_attrs) == len(titles)\n",
    "            for i, title in enumerate(titles):\n",
    "                abbr_dict[title] = simple_attrs[i]\n",
    "        except Exception as e:\n",
    "            # print(f\"{e}\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_datas = json.dumps(abbr_dict, indent=4)\n",
    "with open(\"data/abbreviation_nlm.json\", 'w') as f:\n",
    "    f.write(json_datas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
