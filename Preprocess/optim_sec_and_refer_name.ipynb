{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä¼˜åŒ–ç« èŠ‚åå’Œå‚è€ƒæ–‡çŒ®å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ç« èŠ‚å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55580\n",
      "55580\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../Data/plos_section.csv\")\n",
    "sections = df['section'].tolist()\n",
    "\n",
    "print(len(sections))\n",
    "sections = list(set(sections))\n",
    "print(len(sections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " 'Methodological Considerations and Study Limitations',\n",
       " '2. Flow field model of micro groove seal',\n",
       " '9 Supplementary materials',\n",
       " 'Ethical approval and informed consent']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    1. å¯¹äºä¸è§„èŒƒä½†ç¡®å®å±äºç« èŠ‚åçš„æ•°æ®\n",
    "    2. å¯¹äºä¸è§„èŒƒä¸”ä¸å±äºç« èŠ‚åçš„æ•°æ®  ï¼ˆå¯èƒ½æ˜¯çˆ¬å–è¿‡ç¨‹ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œè¿™éƒ¨åˆ†æ•°æ®å¦‚ä½•å¤„ç†ï¼Œå»é™¤å—ï¼Ÿï¼‰\n",
    "\n",
    "    ä½†é‡ç‚¹æ˜¯å¦‚ä½•åˆ©ç”¨ç« èŠ‚åä¿¡æ¯ğŸ¤”ï¼Œ æ¯”å¦‚è¯´æˆ‘è®¤ä¸ºåœ¨methodéƒ¨åˆ†å…±ç°çš„æ›´åŠ é‡è¦ï¼Œè¿™ä¾æ®ä½•åœ¨ï¼Ÿ\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### å‚è€ƒæ–‡çŒ®å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    1. é€šè¿‡å»ºç«‹å­—å…¸ï¼Œå»ä¼˜åŒ–æœŸåˆŠå\n",
    "    2. é€šè¿‡ä»WOS Scienceè¿›è¡Œæ£€ç´¢è·å–å‚è€ƒæ–‡çŒ®\n",
    "\n",
    "    è¿™ä¸€æ­¥æ˜¯è‚¯å®šè¦åšçš„,ä¸”è¿™ä¸€æ­¥éª¤æ˜¯éœ€è¦é’ˆå¯¹æ‰€æœ‰è®ºæ–‡è¿›è¡Œå¤„ç†çš„\n",
    "\"\"\"\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13024187, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/refer_source.csv')\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = df['Refer_Source'].tolist()\n",
    "\n",
    "title_to_nums = {}\n",
    "for title in sources:\n",
    "    if type(title) != str:\n",
    "        continue\n",
    "    if title not in title_to_nums:\n",
    "        title_to_nums[title] = 0\n",
    "    else:\n",
    "        title_to_nums[title] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887593\n"
     ]
    }
   ],
   "source": [
    "unique_sources = title_to_nums.keys()\n",
    "print(len(unique_sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match = pd.read_csv(\"data/match_journal.csv\")\n",
    "match_journals = df_match['MATCH Source'].tolist()\n",
    "\n",
    "cnt = 0\n",
    "error_cnt = 0\n",
    "error_journals = []\n",
    "for journal in set(match_journals):\n",
    "    try:\n",
    "        cnt += title_to_nums[journal]\n",
    "    except:\n",
    "        error_journals.append(journal)\n",
    "        error_cnt += 1\n",
    "\n",
    "print(cnt)\n",
    "print(cnt/len(sources))\n",
    "print(error_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir(\"data/standard journal list\")\n",
    "\n",
    "journals = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(\"data/standard journal list\", file))\n",
    "    try:\n",
    "        journals.extend(df['Journal title'].tolist())\n",
    "    except:\n",
    "        journals.extend(df['Title'].tolist())\n",
    "        continue\n",
    "\n",
    "scopus_df = pd.read_excel(\"data/standard journal(scopus)/ext_list_August_2024.xlsx\")\n",
    "scopus_df = scopus_df[scopus_df['Source Type'] == 'Journal']\n",
    "\n",
    "journals.extend(scopus_df['Source Title'].tolist())\n",
    "len(set(journals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68121"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_standard = pd.DataFrame()\n",
    "# df_standard['Source'] = list(set(journals))\n",
    "# df_standard.to_csv(\"data/standard_journals.csv\", index=False)\n",
    "\n",
    "df_standard = pd.read_csv(\"data/standard_journals.csv\")\n",
    "stan_journals = df_standard['Source'].tolist()\n",
    "len(set(stan_journals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99302"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ¥è‡ªSciSciNetä¸­çš„æœŸåˆŠå\n",
    "\n",
    "with open(\"data/sciscinet/SciSciNet_Journals.tsv\", 'r', encoding='utf-8') as f:\n",
    "    datas = f.readlines()\n",
    "\n",
    "sci_journals = []\n",
    "for data in datas:\n",
    "    elems = data.split('\\t')\n",
    "    sci_journals.append(elems[1])\n",
    "\n",
    "stan_journals = set(stan_journals + sci_journals)\n",
    "print(len(stan_journals))\n",
    "\n",
    "df_standard = pd.DataFrame()\n",
    "df_standard['Source'] = list(set(stan_journals))\n",
    "df_standard.to_csv(\"data/standard_journals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245324\n"
     ]
    }
   ],
   "source": [
    "# æ¥è‡ªopenalexçš„æœŸåˆŠå\n",
    "import json\n",
    "\n",
    "openalex_journal = []\n",
    "with open(\"data/openalex/journal\", 'r') as f:\n",
    "    datas = json.load(f)\n",
    "\n",
    "openalex_journal = [data['display_name'] for data in datas]\n",
    "print(len(set(openalex_journal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/standard_journals.csv\")\n",
    "stan_journals = df['Source'].tolist()\n",
    "\n",
    "set(stan_journals).issubset(set(openalex_journal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302923\n"
     ]
    }
   ],
   "source": [
    "stan_journals = set(stan_journals+openalex_journal)\n",
    "print(len(stan_journals))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Source'] = list(stan_journals)\n",
    "df.to_csv(\"data/standard_journals_plus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Journal of Psycholinguistic Research\" in stan_journals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### æ ‡å‡†åŒ–å‚è€ƒæ–‡çŒ®å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. è‡ªå·±æ„å»ºç¼©å†™åç§°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸¤ç§æœŸåˆŠåç¼©å†™æ ¼å¼: \n",
    "\n",
    "https://www.sohu.com/a/673180484_121252874\n",
    "\n",
    "https://www.lcgdbzz.org/news/bkgf/58444b6f-f790-4e2d-99d2-38b932ef9ddd.htm\n",
    "\n",
    "    åŸï¼š JOURNAL OF CONTROLLED RELEASE\n",
    "\n",
    "    ç¼©å†™ï¼ˆ1ï¼‰ J CONTROL RELEASE\n",
    "    ç¼©å†™ï¼ˆ2ï¼‰ J.Control. Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "func_words = ['The', 'of', 'the']\n",
    "\n",
    "word_to_abbr = {}\n",
    "\n",
    "def generate_abbreviation(title):\n",
    "    # é¦–å­—æ¯ç¼©å†™ï¼š British Medical Journal   BMJ\n",
    "    words = title.strip().split(\" \")\n",
    "    if len(words) == 1:\n",
    "        return None\n",
    "    \n",
    "    word_list = [word[0] for word in words]\n",
    "    abbr_1 = \"\".join(word_list)\n",
    "\n",
    "    # å»æ‰è™šè¯åçš„é¦–å­—æ¯ç¼©å†™\n",
    "    new_word_list = []\n",
    "    for word in words:\n",
    "        if word in func_words:\n",
    "            continue\n",
    "        else:\n",
    "            new_word_list.append(word[0])\n",
    "    abbr_2 = \"\".join(new_word_list)\n",
    "    \n",
    "    # å»æ‰è™šè¯å¹¶å°†è¯è¯­éƒ¨åˆ†ç¼©å†™\n",
    "    new_word_list = []\n",
    "    for word in words:\n",
    "        if word in func_words or len(word)<=5:\n",
    "            new_word_list.append(word)\n",
    "            continue\n",
    "        if word in word_to_abbr:\n",
    "            new_word_list.append(word_to_abbr[word])\n",
    "        elif word[-3:] == 'ogy' or word[-3:] == 'ics':\n",
    "            new_word_list.append(word[:-3])\n",
    "            word_to_abbr[word] = word[:-3]\n",
    "        elif word[-3:] == 'try':\n",
    "            new_word_list.append(word[:-5])\n",
    "            word_to_abbr[word] = word[:-5]\n",
    "        else:\n",
    "            min_idx = 100\n",
    "            for vowel in vowels:\n",
    "                idx = word.find(vowel)\n",
    "                if idx != -1 and idx != 0 and idx < min_idx:\n",
    "                    min_idx = idx\n",
    "            new_word_list.append(word[:min_idx])\n",
    "            word_to_abbr[word] = word[:min_idx]\n",
    "\n",
    "    abbr_3 = \" \".join(new_word_list)\n",
    "\n",
    "    return abbr_1, abbr_2, abbr_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. åŸºäºç¼–è¾‘è·ç¦»ç®—æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ç”¨ç¬¬ä¸‰æ–¹åº“ï¼š python-Levenshtein, editdistance, textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†ç»“æœè½¬æ¢ä¸ºå­—å…¸ï¼Œä¾¿äºåç»­ä»£ç ä¸­æ£€ç´¢\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"data/close_form_journal.csv\")\n",
    "frame_titles = [\"Raw J\", \"New D\", \"New J\", \"New D Second\", \"New J Second\"]\n",
    "\n",
    "word_to_close_form = {}\n",
    "for row in df.itertuples():\n",
    "    \"\"\"\n",
    "        ç¼–è¾‘è·ç¦»ç›¸å·®å¤ªè¿œçš„å¹¶ä¸èƒ½å½’ä¸ºåŒä¸€ç±»\n",
    "    \"\"\"\n",
    "    try:\n",
    "        word_to_close_form[row[2]] = []\n",
    "        if row[3] < 10:\n",
    "            word_to_close_form[row[2]].append(row[4])\n",
    "        if row[5] < 10:\n",
    "            word_to_close_form[row[2]].append(row[6])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "json_datas = json.dumps(word_to_close_form, indent=4)\n",
    "with open(\"data/close_form_journal.json\", \"w\") as f:\n",
    "    f.write(json_datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. ä»NLMå®˜ç½‘çˆ¬å–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/nlm/part_3_test_results.json\", 'r') as f:\n",
    "    datas = json.load(f)\n",
    "\n",
    "type(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99242\n",
      "99302\n"
     ]
    }
   ],
   "source": [
    "# é¦–å…ˆæ ¹æ®å­—å…¸çš„é”®æ£€æŸ¥æœ‰å“ªäº›urlæ˜¯æ²¡æœ‰æˆåŠŸçˆ¬å–ä¸‹æ¥çš„\n",
    "import pandas as pd\n",
    "\n",
    "crawl_urls = list(datas.keys())\n",
    "\n",
    "df = pd.read_csv(\"data/nlm/standard_journals_nlm_url.csv\")\n",
    "all_urls = df['URL'].tolist()\n",
    "\n",
    "print(len(set(all_urls)-set(crawl_urls)))\n",
    "print(len(set(all_urls)))\n",
    "\n",
    "# miss_df = pd.DataFrame()\n",
    "# miss_df['URL'] = list(set(all_urls)-set(crawl_urls))\n",
    "# miss_df.to_csv(\"data/nlm/miss_urls\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# ç„¶åæ ¹æ®å­—å…¸çš„å€¼æ„å»ºæ ‡å‡†æœŸåˆŠåä¸å…¶ç¼©å†™ä¹‹é—´çš„å…³ç³»\n",
    "from tqdm import tqdm\n",
    "\n",
    "abbr_dict = {}\n",
    "\n",
    "for items in tqdm(datas.values()):\n",
    "    titles = items['title']\n",
    "    try:\n",
    "        attrs = items['attri']\n",
    "    except:\n",
    "        attrs = items['attr']\n",
    "\n",
    "    if type(titles) == str:\n",
    "        abbr_dict[titles] = attrs\n",
    "    else:\n",
    "        simple_attrs = []\n",
    "        for attr in attrs:\n",
    "            if \"NLM Title Abbreviation:\" in attr:\n",
    "                attr = attr.replace(\"NLM Title Abbreviation:\",\"\").strip()\n",
    "                simple_attrs.append(attr)\n",
    "        try:\n",
    "            assert len(simple_attrs) == len(titles)\n",
    "            for i, title in enumerate(titles):\n",
    "                abbr_dict[title] = simple_attrs[i]\n",
    "        except Exception as e:\n",
    "            # print(f\"{e}\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_datas = json.dumps(abbr_dict, indent=4)\n",
    "with open(\"data/abbreviation_nlm.json\", 'w') as f:\n",
    "    f.write(json_datas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
